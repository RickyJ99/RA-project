# -*- coding: utf-8 -*-
"""NLP speech analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ujYPL_-1fmjHl536C0zjuPvI87pcc5S0

# NLP frequency analysis example speech processing
"""

import spacy
import pandas as pd
import warnings
from urllib.request import urlopen
from bs4 import BeautifulSoup
from urllib.error import HTTPError
from datetime import datetime, timedelta
import re
import nltk
from collections import Counter
import gensim
from dateutil import parser
from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer
import sys

warnings.filterwarnings("ignore")

"""## Importing the speech
Starting with the extraction function which takes as input the link and return the text data. The function work with meta and html tag, that can change thus it is good to check the site to not incut in errors.
"""
import sys


def remove_sentence(text):
    sentence = "\n\n\nShare:\n\n\n\n\nShare on Facebook \n\n\n\nShare on Twitter \n\n\n\nCopy URL to your clipboard \n\n\n\n\n\n\n\t\tAll News\t\n\n\n\n"
    if text.startswith(sentence):
        text = text[len(sentence) :]
    return text


def split_text(text):
    match = re.search(r"\s+Q\s+", text)
    if match:
        split_index = match.start()
        return (text[:split_index], text[split_index + len(match.group()) :])
    else:
        return (text, None)


def print_progress_bar(count: int, total: int):
    bar_length = 20
    filled_length = int(round(bar_length * count / float(total)))
    percents = round(100.0 * count / float(total), 1)
    bar = "â–ˆ" * filled_length + "-" * (bar_length - filled_length)
    sys.stdout.write(f"\r[{bar}] {percents}%")
    sys.stdout.flush()


def preprocess(raw_text):
    # regular expression keeping only letters
    letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

    # convert to lower case and split into words -> convert string into list ( 'hello world' -> ['hello', 'world'])
    words = letters_only_text.lower().split()

    cleaned_words = []
    lemmatizer = (
        PorterStemmer()
    )  # plug in here any other stemmer or lemmatiser you want to try out

    # remove stopwords
    for word in words:
        if word not in stop_words:
            cleaned_words.append(word)

    # stemm or lemmatise words
    stemmed_words = []
    for word in cleaned_words:
        word = lemmatizer.stem(
            word
        )  # dont forget to change stem to lemmatize if you are using a lemmatizer
        stemmed_words.append(word)

    # converting list back to string
    return " ".join(stemmed_words)


def split_text_qa(raw_text: str):
    split_index = raw_text.find("Q ")
    if split_index == -1:
        return (raw_text, None)
    else:
        return (raw_text[:split_index], raw_text[split_index + 1 :])


def remove_words(text, words_to_remove):
    for word in words_to_remove:
        text = text.replace(word, "")
    return text


def extract_info(raw_text: str):
    # Extract location
    location_search = re.search(r"\n{3}(.*)\n", raw_text)
    location = location_search.group(1) if location_search else None
    out = extract_info_ai(raw_text)

    if location == None or location.find("Share") != None:
        location = out[3]
    # Extract time
    time_search = re.search(r"\d{1,2}:\d{2}\s[A|P]\.M\.}", raw_text)
    time = time_search.group(0) if time_search else None

    if time_search is None:
        time = out[1]

    date = out[0]  # removing date from the text

    # Extract speaker name
    speaker_search = re.search(r"\n([A-Z\s\.]*):\s", raw_text)
    speaker = speaker_search.group(1).strip() if speaker_search else None
    if speaker_search is None:
        speaker = out[2]

    print(
        "Location: "
        + str(location)
        + ";\nTime: "
        + str(time)
        + ";\nSpeaker: "
        + str(speaker)
        + ";\nDate: "
        + str(date)
    )
    return (location, time, speaker, raw_text)


def extract_info_ai(text):
    # Load a pre-trained NER model
    nlp = spacy.load("en_core_web_sm")

    # reading in the data
    doc = nlp(text)
    # Extract named entities from the text
    entities = [(ent.text, ent.label_) for ent in doc.ents]
    date = []
    time = []
    speaker = []
    location = []

    date_find = False
    time_find = False
    speaker_find = False
    location_find = False

    for count in range(len(entities)):
        if (
            entities[count][1] == "DATE"
            and re.search(r"\d{2}, \d{4}", entities[count][0]) != None
            and len(entities[count][0]) < 27
        ):
            date.append(entities[count][0])
            date_find = True

        elif (
            entities[count][1] == "TIME"
            and re.search(r"\d{1,2}:\d{2}\s[A|P]\.M\.\s[A-Z]{3}", entities[count][0])
            != None
        ):
            time.append(entities[count][0])
            time_find = True

        elif entities[count][1] == "PERSON":
            speaker.append(entities[count][0])
            speaker_find = True

        elif entities[count][1] == "ORG":
            location.append(entities[count][0])
            location_find = True

    if date_find is False:
        date.append(None)

    if time_find is False:
        time.append(None)

    if speaker_find is False:
        speaker.append(None)

    if location_find is False:
        location.append(None)

    return (date[0], time[0], speaker[0], location[0])


def convert_timezone_to_utc(time_str):
    time_str = time_str.replace("\n", "")
    time_str = time_str.replace("EEST", "EST")
    time_str = time_str.strip()
    # Extract timezone from input string
    timezone = time_str[-3:]

    # Convert string to datetime object
    date_string = time_str[:-4]
    date_string = date_string.replace(".", "")

    dt_obj = datetime.strptime(date_string, "%I:%M %p")

    # Subtract hours based on timezone
    if timezone in "EST" or "E":
        dt_obj -= timedelta(hours=5)
        timezone = "+00:00"
    elif timezone in "CST":
        dt_obj -= timedelta(hours=6)
        timezone = "+00:00"
    elif timezone in "EDT":
        dt_obj -= timedelta(hours=4)
        timezone = "+00:00"
    elif timezone in "PDT":
        dt_obj -= timedelta(hours=7)
        timezone = "+00:00"
    elif timezone in "MST":
        dt_obj -= timedelta(hours=7)
        timezone = "+00:00"
    elif timezone in "MDT":
        dt_obj -= timedelta(hours=6)
        timezone = "+00:00"
    else:
        print("Timezone exe: " + timezone)
        dt_obj -= timedelta(hours=0)
        timezone = "+00:00"

    # Add UTC timezone to datetime object
    dt_obj = dt_obj.replace(tzinfo=datetime.strptime(timezone, "%z").tzinfo)

    # Return datetime object in desired format
    return dt_obj.strftime("%I:%M %p. %Z")


def combine_date_and_time(date_string, time_string):
    time_string = convert_timezone_to_utc(time_string)
    date_object = datetime.fromisoformat(date_string)
    time_object = datetime.strptime(time_string, "%I:%M %p. %Z").time()
    new_datetime = datetime.combine(date_object.date(), time_object)
    new_datetime = str(new_datetime) + "+00:00"
    new_datetime = new_datetime.replace(" ", "T")
    print(new_datetime)
    return new_datetime


speech_df = pd.read_csv("Full.csv", sep=",")

"""This is the dataframe that has been imported from the csv file

## Removing Duplicates
"""

# let's remove duplicates first

speech_df["dup"] = speech_df.duplicated(subset=None, keep="first")

speech_df.head()

"""We have created a new column that stores a boolean value whether the row is a duplicate row or not. We can see that for the second Sherlock Holmes the value in that column is True. We want to delete those rows where dup == True"""

speech_df = speech_df[speech_df["dup"] == False]


del speech_df["dup"]  # deleting "dup" column since we don't need it anymore

"""## Preprocessing

Great, it has worked. Let's do some text preprocessing.
"""


""" extrapolation from the main text the most import info"""
# eliminatig words sentence that it has been generated downloading from the sites
with open("words_to_remove.txt", "r") as f:
    words_to_remove = [line.strip() for line in f]


# Add the new columns using the assign method
speech_df["Location/Organization"] = None
speech_df["Speaker"] = None
speech_df["QA"] = None

for count in range(len(speech_df["Main text"])):
    print(str(count) + "/" + str(len(speech_df["Main text"])) + "\n")
    print_progress_bar(count, len(speech_df["Main text"]))
    print("\n")
    # recalling the function to eliminate misleading sentece
    speech_df["Main text"][count] = remove_words(
        speech_df["Main text"][count], words_to_remove
    )
    raw_text = speech_df["Main text"][count]
    speech_df["Main text"][count] = remove_sentence(raw_text)
    speech_df["Main text"][count], speech_df["QA"][count] = split_text(
        speech_df["Main text"][count]
    )
    # split QA session
    out = split_text_qa(speech_df["Main text"][count])
    speech_df["QA"][count] = out[1]
    speech_df["Main text"][count] = out[0]

    out = extract_info(speech_df["Main text"][count])  # extraction of the info
    speech_df["Location/Organization"][count] = out[0]  # location

    # time info
    if speech_df["Administration"][count] != "Biden":
        time_to_add = out[1]
        if time_to_add is not None:
            speech_df["Date Time"][count] = combine_date_and_time(
                speech_df["Date Time"][count], time_to_add
            )

    # Speaker
    speech_df["Speaker"][count] = out[2]

    # Main text
    speech_df["Main text"][count] = out[3]

speech_df.to_csv("Extraction.csv", index=False)
print(speech_df.head())
