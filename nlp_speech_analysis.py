# -*- coding: utf-8 -*-
"""NLP speech analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ujYPL_-1fmjHl536C0zjuPvI87pcc5S0

# NLP frequency analysis example speech processing
"""

import pandas as pd
import warnings
from urllib.request import urlopen
from bs4 import BeautifulSoup
from urllib.error import HTTPError
from datetime import datetime
import re
import nltk

nltk.download("wordnet")
from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer

warnings.filterwarnings("ignore")

"""# Download the data from the white house 
Importing all the links from https://www.whitehouse.gov/briefing-room/press-briefings/ 
"""

base_url = "https://www.whitehouse.gov/briefing-room/press-briefings/page/"
page_number = 1
all_links = []
# Biden administration
while True:
    try:
        url = base_url + str(page_number)
        html = urlopen(url).read()
        soup = BeautifulSoup(html, features="html.parser")
        urls = soup.find_all("a", class_="news-item__title")
        for url in urls:
            all_links.append(url["href"])
        page_number += 1
    except HTTPError:
        break

# Trump administration
base_url = "https://trumpwhitehouse.archives.gov/briefings-statements/page/"
page_number = 1

while True:
    try:
        url = base_url + str(page_number)
        html = urlopen(url).read()
        soup = BeautifulSoup(html, features="html.parser")
        urls = soup.find_all("h2", class_="briefing-statement__title")
        for url in urls:
            all_links.append(url.a["href"])
        page_number += 1
    except HTTPError:
        break

with open("url.txt", "w") as f:
    for link in all_links:
        f.write(link + "\n")

"""## Importing the speech
Starting with the extraction function which takes as input the link and return the text data. The function work with meta and html tag, that can change thus it is good to check the site to not incut in errors.
"""


def extr(url, adm):
    print("----------------------------------------------------")
    print("----------------------------------------------------")
    print(url)
    html = urlopen(url).read()
    soup = BeautifulSoup(html, features="html.parser")
    # kill all script and style elements
    for script in soup(["script", "style"]):
        script.extract()  # rip it out

    # get text
    text = soup.get_text()
    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = "\n".join(chunk for chunk in chunks if chunk)

    if adm == 1:  # biden administration
        # get metas
        # extract title
        title = soup.find("meta", property="og:title")
        title = title["content"]
        print("Title:" + title)

        # extract description
        desc = soup.find("meta", property="og:description")
        desc = desc["content"]
        print("Description:" + desc)

        # extract datatime
        date = soup.find("meta", property="article:published_time")
        date_str = str(date["content"])

        # convert the date in the correct format
        date = datetime.fromisoformat(date_str)
        print("Date:" + str(date))

        # extract text
        body = soup.find("section", class_="body-content")
        body = body.text
        print("Text extracted")
    elif adm == 0:
        # extract title
        title = soup.find("h1", {"class": "page-header__title"}).text
        print("Title:" + title)

        # extract date
        date_str = soup.find("time").text
        print("Description:" + desc)
        # convert the date in the correct format
        date_obj = datetime.strptime(date_str, "%B %d, %Y")
        date = date_obj.isoformat() + "+00:00"
        print("Date:" + str(date))

        # extract description
        # no desc with the trump

        # extract text
        body = soup.find("div", class_="page-content__content editor")
        body = body.text
        print("Text extracted")

    if title is None:
        title = "Nan"
    if desc is None:
        desc = "Nan"
    if date is None:
        date = "Nan"
    if body is None:
        date = "Nan"
    print("----------------------------------------------------")
    print("----------------------------------------------------")
    out = [date, title, desc, body, url]
    return out


"""

1.   Reading the text file with the links
2.   Recall the extrapolation function for each link
3.   Saving the data in a csv file

"""

# reading the file with the link
f = open("url.txt", "r")
url1 = f.readlines()


# print(speech_text)
# create a data frame
# initialize variables
date = []
title = []
desc = []
speech_text = []
link = []
admin = []

# download the data
for url_count in url1:
    print("Analizing:" + url_count)
    if "https://www.whitehouse.gov/" in url_count:  # biden adm
        out = extr(str(url_count), 1)  # recalling function extrapolation
        admin.append("Biden")
    elif "https://trumpwhitehouse.archives.gov/" in url_count:
        out = extr(str(url_count), 0)  # recalling function extrapolation
        admin.append("Trump")
    # append the results
    date.append(out[0])
    title.append(out[1])
    desc.append(out[2])
    speech_text.append(out[3])
    link.append(out[4])


columns_name = [
    "Administration",
    "Date Time",
    "Title",
    "Description",
    "Main text",
    "URL",
]

df_imported = pd.DataFrame(
    list(zip(admin, date, title, desc, speech_text, url)), columns=columns_name
)


# save in a csv file
df_imported.to_csv("Book1.csv", index=False)

"""Thus I save a csv fil called Book1.csv. Now I open and import the data and starting the text analysis """

# reading in the data

speech_df = pd.read_csv("/content/Book1.csv", sep=",")
speech_df

"""This is the dataframe that has been imported from the csv file

## Removing Duplicates
"""

# let's remove duplicates first

speech_df["dup"] = speech_df.duplicated(subset=None, keep="first")

speech_df.head()

"""We have created a new column that stores a boolean value whether the row is a duplicate row or not. We can see that for the second Sherlock Holmes the value in that column is True. We want to delete those rows where dup == True"""

speech_df = speech_df[speech_df["dup"] == False]

speech_df

del speech_df["dup"]  # deleting "dup" column since we don't need it anymore

"""## Preprocessing

Great, it has worked. Let's do some text preprocessing.
"""

cols = [0, 1, 2, 4]  # column indexes we dont need to analyse
speech_df.drop(speech_df.columns[cols], axis=1, inplace=True)
speech_df


stop_words_file = "/content/SmartStopList.txt"

stop_words = []

with open(stop_words_file, "r") as f:
    for line in f:
        stop_words.extend(line.split())

stop_words = stop_words


def preprocess(raw_text):
    # regular expression keeping only letters
    letters_only_text = re.sub("[^a-zA-Z]", " ", raw_text)

    # convert to lower case and split into words -> convert string into list ( 'hello world' -> ['hello', 'world'])
    words = letters_only_text.lower().split()

    cleaned_words = []
    lemmatizer = (
        PorterStemmer()
    )  # plug in here any other stemmer or lemmatiser you want to try out

    # remove stopwords
    for word in words:
        if word not in stop_words:
            cleaned_words.append(word)

    # stemm or lemmatise words
    stemmed_words = []
    for word in cleaned_words:
        word = lemmatizer.stem(
            word
        )  # dont forget to change stem to lemmatize if you are using a lemmatizer
        stemmed_words.append(word)

    # converting list back to string
    return " ".join(stemmed_words)


test_sentence = (
    "this is a sentence to demonstrate how the preprocessing function works...!"
)

preprocess(test_sentence)

"""you can see that "sentence" was stemmed to "sentenc", all stop words and punctuation were removed.
Let's apply that function to the incident texts in our speech dataframe
"""

speech_df["prep"] = speech_df["Main text"].apply(preprocess)

speech_df.head()

"""## Most Common Words
In order to get an idea about a dataset, it's useful to have a look at the most common words. Reading through all incident texts is cumbersome and inefficient. Let's extract the most common key words

"""

from collections import Counter

Counter(" ".join(speech_df["prep"]).split()).most_common(10)

# Commented out IPython magic to ensure Python compatibility.
# nice library to produce wordclouds
from wordcloud import WordCloud

import matplotlib.pyplot as plt

# if uising a Jupyter notebook, include:
# %matplotlib inline

all_words = ""

# looping through all incidents and joining them to one text, to extract most common words
for arg in speech_df["prep"]:
    tokens = arg.split()

    all_words += " ".join(tokens) + " "

wordcloud = WordCloud(
    width=700, height=700, background_color="white", min_font_size=10
).generate(all_words)

# plot the WordCloud image
plt.figure(figsize=(5, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)

plt.show()

from nltk.util import ngrams

n_gram = 2
n_gram_dic = dict(Counter(ngrams(all_words.split(), n_gram)))

for i in n_gram_dic:
    if n_gram_dic[i] >= 2:
        print(i, n_gram_dic[i])
